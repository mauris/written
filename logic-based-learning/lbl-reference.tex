\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[nodayofweek]{datetime}
\longdate
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{comment}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{float}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers

\setlength\columnsep{30pt}

\geometry{
 	a4paper,
	total={170mm,257mm},
 	left=20mm,
 	top=20mm,
}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\title{
	 \huge 304: Logic-Based Learning\\
	 \huge -- Reference --
}
\date{\today}
\author{
	Sam Yong \\
	\small \href{mailto:sam.yong17@imperial.ac.uk}{sam.yong17@imperial.ac.uk}
}


\begin{document}
\maketitle

\begin{multicols}{2}

\paragraph{} NOTICE: This set of reference is currently incomplete with respect to the syllabus of the course and will be updated as the semester progress. By end Mar 2018 it should be complete.

\section*{Foreword}  

\paragraph{} This reference was made as an condensation from the lecture slides and notes provided by Prof. Alessandra Russo, Prof. Stephen Muggleton and Mark Law in the Imperial College London, Department of Computing's 304: Logic-Based Learning.

\paragraph{} The ordering of this reference may not correspond to the sequence introduced in the lectures, lecture slides and notes. This order is how I feel I would understand the topic better.

\begin{footnotesize}
\paragraph{License} This reference is made publicly available under the MIT License. You should not have paid anyone money in exchange for this document. If you have paid someone for it, well too bad. The source code for this document can be found public available on my Github repository\footnote{\href{https://github.com/mauris/written}{https://github.com/mauris/written}}. If you wish to help improve this document, feel free to open an issue on the Github repository.
\end{footnotesize}

\tableofcontents
\newpage

\section{Background}
\subsection{Forms of Reasoning}

\paragraph{Deduction} Reasoning from the general to reach the particular: what follow necessarily from the given premises.

\begin{itemize}
\item \textbf{Rule}: All beans in this bag are white.
\item \textbf{Case}: These beans are from this bag.
\item \textbf{Result}: These beans are white.
\end{itemize}

\paragraph{Induction} Reasoning from the specifics to reach the general: process of deriving reliable generalisations from observations.

\begin{itemize}
\item \textbf{Rule}: These beans are from this bag.
\item \textbf{Case}: These beans are white.
\item \textbf{Result}: All beans in this bag are white.
\end{itemize}

\paragraph{Abduction} Reasoning from observations to explanations: process of using given general rules to establish causal relationships between existing knowledge and observations. 

\begin{itemize}
\item \textbf{Rule}: All beans in this bag are white.
\item \textbf{Case}: These beans are white.
\item \textbf{Result}: These beans are from this bag.
\end{itemize}

\subsection{Clausal Representation}
\subsubsection{Propositional Logic}
\paragraph{} We begin by introducing the clausal representation with the following definitions:

\begin{itemize}
\item \textbf{Theory}: a set (conjunction) of clauses\footnote{Clausal finite theories can also be seen as Conjunctive Normal Form (CNF) formulae.}.\\ e,g, $\{p \lor \lnot q; r; s\}$
\item \textbf{Clause}: disjunction of literals.\\ e.g. $p \lor \lnot q$, $r$, $s$
\item \textbf{Literal}: Atomic sentence or its negation.\\ e.g. $p$, $\lnot q$
\end{itemize}

\paragraph{} As we want to explore how inference tasks can be computed, we will restrict ourselves to the subset of predicate logic that is computational tractable and where efficient automated proof procedures that are able to compute logical inference exists. This subset of predicate logic is called Horn clauses\footnote{A subset of Prolog language.}. Every formula can be converted into a clausal theory:

\begin{itemize}
\item Elimination of $\implies$:\\ $(p \implies q) \longrightarrow \lnot p \lor q$
\item Pushing $\lnot$ inwards:\\ $\lnot(p \lor q) \longrightarrow (\lnot p \land \lnot q)$
\item Distribution of $\land$ and $\lor$:\\ $(\lnot p \land \lnot q) \lor \lnot p \longrightarrow (\lnot p \lor \lnot p) \land (\lnot q \lor \lnot p)$
\item Collecting Terms:\\ $(\lnot p \lor \lnot p) \land (\lnot q \lor \lnot p) \longrightarrow \lnot p \land (\lnot q \lor \lnot p)$
\end{itemize}

\subsubsection{Predicate Logic}

\paragraph {} In the case of predicate logic, atomic sentences may have terms with variables:

\begin{itemize}
\item \textbf{Theory}: a set (conjunction) of clauses.\\ e.g. $\{p(X) \lor \lnot r(a, f(b, X)); q(X, Y)\}$\\ All variables are understood to be universally quantified, i.e.\\ $\forall X, Y [r(a, f(b, X) \implies p(X)] \land \forall X, Y\ q(X, Y)$
\end{itemize}

\paragraph{Substitution} Since terms may have variables in predicate logic, substitution is needed to ground literals. For example, let $\theta = \{V_1/t_1, V_2/t_2, ..., V_n/t_n\}$ where $V_i$ is a variable and $t_i$ a term that replaces $V_i$. Suppose, 

\begin{align*}
&p(X, Y)\\
\theta &= \{X/a, Y/g(b, Z)\}\\
p(X, Y)\theta &= p(a, g(b, Z))
\end{align*}

\paragraph{Grounding} A literal is \textit{ground} if it contains no variables.

\paragraph{Instance} A literal $l'$ is \textit{an instance of} $l$, if for some substitution $\theta$, $l' = l\theta$.

\paragraph{Skolemisation} When converting FOL to CNF, we also need to perform skolemisation to remove existential quantifiers and move all universal quantifiers to the front. 

\begin{itemize}
\item An existential quantifier with no dependency can be skolemised by introducing a new constant. For example, $\exists X\ p(X) \longrightarrow p(c)$. The constant $c$ that is replacing $X$ must not clash in name with other existing constants.
\item If an existential quantifier depends on another quantifier, a function symbol needs to be introduced. For example, $\forall X \exists Y p(X, Y) \longrightarrow \forall p(X, f(X))$.
\end{itemize}

\noindent All universal quantifiers need to be moved to the front. Variable name clashes must be resolved by renaming variables as the universal quantifiers are being moved to the front. Once they are at the front, all universal quantifiers at the front can all be removed. 

\subsection{Horn Clauses}

\begin{defn}Horn Clauses are a particular type of clauses with at most one positive literal only. The positive literal in the clauses is called the head and the other literals form the body of the clause.\end{defn}

\begin{defn}Definite Clauses have exactly one positive literal.  e.g. $\lnot b_1 \lor\lnot b_2 \lor ... \lor\lnot b_n \lor h$. Definite Clauses (aka rules) can also be re-written as $h \Leftarrow b_1 \land b_2 \land ... \land b_n$.\end{defn}

\begin{defn}Denials have no positive clauses. e.g. $\lnot b_1 \lor\lnot b_2 \lor ... \lor\lnot b_n$. Denials (aka constraints) can also be re-written as $\Leftarrow b_1 \land b_2 \land ... \land b_n$.\end{defn}

\begin{defn}Facts are Definite Clauses that have no negative literals. e.g. $h$. We omit the $\leftarrow$  or \lstinline{:-} symbols as with convention for facts.\end{defn}

\subsection{Resolution}

\paragraph{} Resolution is a proof strategy to determine if a proposition can be satisfied by a clausal theory. Essentially, we will see that resolution is actually proof by contradiction (or in Latin, \textit{reductio ad absurdum} abbreviated as RAA).

\subsubsection{Propositional Logic Resolution}

\paragraph{Resolvent} Given two clauses of the form\\ $\{p \lor C_1; \lnot p \lor C_2\}$, the clause $C_1 \lor C_2$ is the inferred clause, called the \textit{resolvent} (conclusion of the premise).

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{graphics/propositional-logic-resolution-example1.png}
\caption{In this example, $\lnot r$ and $r$ gets cancelled away and resolves the two statement $w \lor r \lor q$ and $w \lor s \lor \lnot r$ into $w \lor q \lor s$}
\end{figure}

\paragraph{Refutation Completeness} Resolution is complete as a refutation system. That is, if $S$ is a contradictory set of clauses, then resolution can refute $S$, i.e. $S \vdash [\ ]$ or $S \vdash \perp$. For example given the case of $p \models p \lor q$, resolution cannot be directly applied to the given clausal theory $Th = \{p\}$ and infer $p \lor q$. Instead, we have to express it as a refutation problem and show that $\{p, \lnot(p \lor q)\} \models [\ ]$. We then show that the set of clauses $\{p, \lnot p, \lnot q\}$ converted from $Th$ can derive the empty clause $[\ ]$.

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{graphics/propositional-logic-resolution-example2.png}
\caption{In the example to show $p \models p \lor q$, we need to first express it as a refutation problem then derive the empty clause using resolution. Not all clauses need to be used.}
\end{figure}

\paragraph{Resolution} Given a knowledge base of clauses $Kb$, if $Kb \vdash c$ by resolution then $Kb \models c$. 

\begin{exmp}For example,
\begin{align*}
Kb &= \{\\
	&s;\\
	&s \implies c;\\
	&c \land m \implies b;\\
	&c \land f \implies g;\\
	&f\\
	\}&
\end{align*}

\noindent To prove that $Kb \models g$, we show that $Kb \cup \{\lnot g\} \models [\ ]$:

\begin{figure}[H]
\centering
\includegraphics[width=0.32\textwidth]{graphics/propositional-logic-resolution-example3.png}
\caption{By resolution we show that $Kb \cup \{\lnot g\} \models [\ ]$, hence $Kb \models g$.}
\end{figure}\end{exmp}

\subsubsection{Predicate Logic Resolution}

\paragraph{} Resolution in FOL is more complex: while it is still based on the idea of resolving opposite literals that appear in two clauses and deriving the empty clause, variables play an important role here. Literals may have unground terms (i.e. variables that yet to be substituted) which are understood as standing for all possible instances. Resolution could happen by referring to any such instances. Hence the role of the unification step is to identify which of these instances to use.

\paragraph{Name Clashes} When resolving two clauses, all variables occurring should be renamed with unused variables to avoid name clashes. A variable $X$ in clause $C_1$ is not the same as another variable named $X$ in $C_2$.

\paragraph{Resolution} Consider two opposite literals in two clauses which we want to resolve: We see if it is possible to find a substitution $\theta$ st when applied to both literals, $\theta$ makes the two literals equal. The resolution can then applied and the substitution has to be applied on all occurrences of those variables in the literals left in the resolvent. For example, suppose

\begin{align*}
Kb &= \{\\
	&on(a, b);\\
	&on(b, c);\\
	&green(a);\\
	&\lnot green(c)\\
	\}&
\end{align*}

\noindent and we want to show that $Kb \models \exists X \exists Y (on(X, Y) \land green(X) \land \lnot green(Y))$, we can apply the following resolution:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{graphics/predicate-logic-resolution-example1.png}
\caption{The resolution takes the two substitutions (labelled blue) two create equally opposing literals for $on(X, Y)$.}
\end{figure}

\paragraph{} As we take the negation of the entailment, the existential quantifiers get eliminated and hence there is no need for skolemisation.

\paragraph{Answers to Query} Answer to the query may also return unification values (substitution). However, it is possible to construct resolutions that never terminate.

\subsection{Herbrand Theorem}

\paragraph{} It is possible to handle some predicate logic cases by converting them into propositional logic form. Consider $Th$ be a set of clauses (a clausal theory).

\paragraph{Herbrand Domain}\footnote{Also known as Herbrand Universe.} The set of all ground terms formed only using constants and function symbols that appear in $Th$.

\paragraph{Herbrand Base} The set of all ground atoms that can be formed using the predicate symbols in $Th$ and terms in the Herbrand Domain.

\paragraph{Grounding} The set of all $c_i\theta$ ground clauses st $c_i \in Th$ and the substitution $\theta$ replaces variables in $c_i$ by terms in the Herbrand Domain. The grounding of $Th$ is denoted as $ground(Th)$.

\paragraph{Theorem} A clausal theory $Th$ is \textit{satisfiable} iff the grounding of $Th$ is \textit{satisfiable}. Since the grounding of $Th$ has no variables, it is essentially propositional. $Th$ is not satisfiable iff by resolution the grounding of $Th$ concludes to the empty clause.

\begin{exmp}\label{exmp:HerbrandTheorem} Consider FOL sentence $S$ whose language $\mathcal{L}$ includes only the constants $b$, $c$, $l$:\end{exmp}

$$S = \forall X, Y [\lnot p(b, Y) \lor p(c, l) \lor (p(b, X)\lor \lnot p(X, l))]$$

\noindent From $S$ we can build the set $C$ of clauses st $C = \{\lnot p(b, Y); p(c,l); p(b, X) \lor \lnot p(X, l)\}$. The \textit{Herbrand Domain of $C$} is $\{b, c, l\}$ while the grounding is:

\begin{align*}
ground(C) &= \{\\
	&\lnot p(b, b),\\
	&p(b, b), \lor \lnot p(b, l),\\
	&\lnot p(b, c),\\
	&p(b, c) \lor \lnot p(c, l),\\
	&\lnot p(b, l),\\
	&p(b, l) \lor \lnot p(l, l),\\
	&p(c, l)\\
	\}&
\end{align*}

\paragraph{} In Example \ref{exmp:HerbrandTheorem}, $ground(C)$ is not satisfiable because of a contradicting subset of the ground clauses: $\{\lnot p(b, c), p(c, l), p(b, c) \lor\lnot p(c, l)\}$. Hence the set of clauses $C$ is unsatisfiable.


\paragraph{Herbrand Interpretation} A Herbrand Intepretation (HI) of a set $Th$ of definite clauses is a set of ground atoms over the constant, function and predicate symbols occurring in $Th$. In essence, a HI is a subset of the Herbrand Base of $Th$, and the set of all Herbrand Interpretations of $Th$ is the power set of the Herbrand Base.

\paragraph{Herbrand Model} A Herbrand Interpretation $I$ is a Herbrand Model (HM) of $Th$ iff for all clauses $\lnot b_1 \lor \lnot b_2 \lor ... \lor \lnot b_n \lor h_1 \lor h_2 \lor ... \lor h_m$ in $Th$ and ground substitutions $\theta$, $$\{b_1\theta, b_2\theta, ..., b_n\theta\} \in I \implies \{h_1\theta, h_2\theta, ..., h_m\theta\} \cap I \not= \emptyset$$

\noindent The inequality with an empty set denotes that the HI $I$ must satisfy $\exists i \in [1, m]\ h_i\theta$. Hence a Herbrand Model is a Herbrand Interpretation is satisfable in a clausal theory.

\begin{defn} Since there may be more than one HI that satisfies a clausal theory (i.e. more than one HM), some HM may be a subset of another HM. The \textit{Minimal Herbrand Model} is a HM of which none of its subsets is a HM. Any satisfiable clausal theory $Th$ of definite clauses has one unique minimal HM called the \textit{Least Herbrand Model}. \end{defn}

\paragraph{Infinite Herbrand Domain} It is possible for a clausal theory to accept an infinite HM as they may have an infinite HD. Consider the following example which has an infinite HD:

\begin{align*}
Th &= \{\\
	&\text{natural}(0),\\
	&\text{natural}(X) \implies \text{natural}(succ(X))\\
	\}&
\end{align*}

\subsection{SLD Derivation}
\paragraph{} A pure Prolog program is a set of definite clauses and hence its semantics is given by its Least Herbrand Model\footnote{Defined by Luc De Raedt in his book.}. The inference procedure used by Prolog is a special version of resolution that exploits the fact that the given clausal theory is a set of definite clauses and not general clauses. 

\paragraph{} Queries to a Prolog program are not any arbitrary clauses but denial clauses\footnote{We want to proof by refutation.}. Arbitrary clauses must be rewritten in a certain way to accomodate this syntactic restriction of Prolog. 

\paragraph{} Given a set of definite clauses and a denial clause, there can be many resolution proofs that can be constructed. Prolog uses a special form of resolution - SLD resolution - to systematically explore all possible derivations. The SLD derivation is a linear sequence of application of an SLD inference rule that is applied between a denial clause and a definite clause in the original $Th$ clausal theory. SLD is a resolution proof method and as such is still refutation based. Hence the denial clause is the given query expressed in denial form. 

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{graphics/sld-inference-rule.png}
\caption{$\theta$ in the inference rule is the Most General Unifier $mgu(\alpha_1, \alpha_1')$. $\alpha_i$ and $\beta_j$ are atoms.}
\end{figure}

\paragraph{} Given a denial (goal) $G_0$ and clausal theory $Th$ of definite clauses, an SLD-derivation of $G_0$ from $Th$ is a (possibly infinite) sequence of denials:

$$G_0 \underset{C_0}{\implies} G_1 \underset{C_1}{\implies} ...  \underset{C_{n-2}}{\implies} G_{n - 1}\underset{C_{n - 1}}{\implies} G_n  $$

\noindent ... where $G_{i + 1}$ is derived directly from $G_i$ and a clause $C_i$ with variables appropriate renamed. This means the composition $\theta = \theta_1 \theta_2 ... \theta_n$, where $\theta_i$ is defined at each step of the derivation, gives the entire substitution computed reaching the final derivation if computation is \textit{finite}.

\subsubsection{SLD Trees}

\paragraph{} When performing SLD derivation, there may be one or more choices of clause $C_i$ that can be used with $G_i$ to derive $G_{i+1}$. Consider the following knowledge base and the query $\exists Z\ \text{proud}(Z)$:

\begin{itemize}
\setlength\itemsep{0.0em}
\item[] $\text{proud}(X) \Leftarrow \text{parent}(X, Y), \text{newborn}(Y)$
\item[] $\text{parent}(X, Y) \Leftarrow \text{father}(X, Y)$
\item[] $\text{parent}(X, Y) \Leftarrow \text{mother}(X, Y)$
\item[] $\text{father}(\text{adam}, \text{mary})$
\item[] $\text{newborn}(\text{mary})$
\end{itemize}


\begin{figure}[H]
\centering
\includegraphics[width=0.45\textwidth]{graphics/sld-derivation-example.png}
\caption{By SLD derivation, we determine that $Z = \text{adam}$.}
\end{figure}

\paragraph{} However we can see that at $C_2$, another clause $\text{parent}(X, Y) \Leftarrow mother(X, Y)$ could have been picked instead for the reservation. When each sub-goal can unify with more clauses, more than one SLD derivations can be computed. Alternative choices can then be represented as a tree, with each leaf as a possible derivation.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{graphics/sld-tree.png}
\caption{Each time there is a choice of clauses can be be used to resolve the subgoals, branches are created in the tree to represent the different possible derivations. In this case, picking $\Leftarrow \text{mother}(X, Y), \text{newborn}(Y)$ as $G_2$ would lead to a non-empty derivation. }
\end{figure}

\paragraph{} The SLD Tree represents the search space of all possible derivations. Each path from the original denial $G_0$ to the leaf node is the SLD computation / refutation. The tree is then traversed in a depth-first search to provide each refutation in order. For a finite SLD-tree, this strategy is complete. Whenever the traversal reaches a leaf node of an empty clause, the substitution of the completed refutation is returned. As mentioned before, it is also possible for SLD derivation to generate an infinite sequence of denials and hence an infinite SLD-tree, which explains why sometimes Prolog computations do not terminate.

\subsection{Normal Clausal Logic}\label{sec:NormalClausalLogic}

\paragraph{Normal Clausal Logic} extends Horn Clauses by permitting atoms in the body of a rule / denial to be prefixed with a special operator \textit{not} (read as "fail"). This operator is also referred to as "negation by failure". For example:

\begin{itemize}
\setlength\itemsep{0.1em}
\item[] \textbf{Normal Clauses}:\\ $h \Leftarrow b_1, ... b_n, \text{not}\ b_{n+1}, ..., \text{not}\ b_m$
\item[] \textbf{Normal Denials}:\\ $\Leftarrow b_1, ... b_n, \text{not}\ b_{n+1}, ..., \text{not}\ b_m$
\end{itemize}

\paragraph{} The \textit{not} operator in Prolog is known as $\backslash+$. The computational meaning of $\text{not}\ p$ is: i) $\text{not}\ p$ succeeds iff $p$ fails finitely and ii) $\text{not}\ p$ fails iff $p$ succeeds. When evaluating $\Leftarrow \text{not} p(X)$, the atom $p$ must be ground: otherwise the derivation is "floundered"\footnote{The answer is unknown.}. The variable $X$ should have already been grounded in previous steps of the derivation.

\paragraph{} In Normal Clausal Logic, the fail operator never appears in the head of a rule. Putting the fail operator in the head of a rule would be asking to prove what should not be proved, whereas clausal theories define what should be provable.

\subsection{SLDNF Derivation}
\paragraph{} SLDNF derivation is SLD derivation with Negation as Failure. A selected subgoal $\text{not}\ p)$,  succeeds if the subproof fails, and it fails if the subproof succeeds. Consider the following example knowledge base and query $connected$:

\begin{itemize}
\setlength\itemsep{0.1em}
\item[] $connected \Leftarrow \text{not}\ unconnected$
\item[] $unconnected \Leftarrow node(X), \text{not}\ succ(X)$
\item[] $succ(X) \Leftarrow arc(X, Y)$
\item[] $node(a)$
\item[] $node(b)$
\item[] $arc(a, b)$
\item[] $arc(b, c)$
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{graphics/sldnf-example1.png}
\caption{ In the example, we need to prove that $\Leftarrow unconnected$ fails in order to prove the second denial / sub-goal "$\Leftarrow \text{not}\ unconnected$". $\Leftarrow unconnected$ fails iff all all possible outcomes of $\Leftarrow unconnected$ comes to a failure / non-empty clause. The two possible outcomes, for $X = a \text{or} b$: $\Leftarrow \text{not}\ succ(X)$, are proven to to fail since each of $\Leftarrow succ(X)$ succeeds.}
\end{figure}

\paragraph{Floundering} If we make a slight modification to the knowledge base of the example by replacing $unconnected \Leftarrow node(X), \text{not}\ succ(X)$ with $unconnected \Leftarrow node(X), \text{not} arc(X, Y)$, the SLDNF would not be possible to evaluate even though the knowledge bases are equivalent. Figure \ref{fig:sldnf-example2} shows the resolution tree.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{graphics/sldnf-example2.png}
\caption{With the slight modification, it becomes impossible to prove $\text{not}\ arc(X, Y)$ for $a$ and $b$ since $Y$ was not instantiated, as it would require a search through possibly infinite derivations to prove that there is exists no such value of $Y$. This is why the SLDNF strategy adopted is whenever a non-ground fail literal is encountered as a subgoal, the derivation consider it not possible to evaluate and a floundering condition is reported.}
\label{fig:sldnf-example2}
\end{figure}

\subsection{Abduction}

\paragraph{} So far reasoning has been primarily deductive. Sometimes our knowledge base can be incomplete and deductive inference would fail on certain queries due to lack of information. For example, given the following knowledge base and the query $soreElbow$:

\begin{itemize}
\setlength\itemsep{0.1em}
\item[] $soreElbow \Leftarrow tennisElbow$
\item[] $tennisPlayer \Leftarrow tennisElbow$
\item[] $soreElbow \Leftarrow soreJoints$
\item[] $soreJoint \Leftarrow arthritis, untreated$
\end{itemize}

\paragraph{} It is impossible to explain $soreElbow$ using deductive inference, as shown in Figure \ref{fig:abduction-example1} below.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{graphics/abduction-example1.png}
\caption{}
\label{fig:abduction-example1}
\end{figure}

\paragraph{} Abductive reasoning computes explanations that are consistent with the given knowledge base to explain (or satisfy) the given observations. The list of explanations can be generated and some may be subset of others. They should not be unnecessarily strong\footnote{i.e. including more ground literals than needed} or unnecessarily weak \footnote{too few to prove the observations in all circumstances}. Abductive algorithms for Normal clauses aim to generate a minimal set of explanations that, together with the given theory, proves the goal.

\paragraph{Abducibles} When modelling a problem domain in an abductive term, we need to think about a vocabulary of what are considered to be plausible explanations for the given types of observations - and we refer this vocabulary as the set of \textit{abducibles}.

\subsubsection{Abductive Model}

\paragraph{} An abductive model of a problem domain is defined as a tuple $<KB, Ab, IC>$ where $KB$ is the knowledge base (set of normal clauses), $Ab$ is the vocabulary of plausible explanations (set of ground undefined literals), and $IC$ is the set of constraints (set of normal details).

\paragraph{Abductive Solution} Given an abductive model, an \textit{abductive solution} (or explanation) of a given observation $O$ is a set $\Delta$ of ground literals st:

\begin{itemize}
\item $\Delta \subseteq Ab$: $\Delta$ belongs to a predefined language of abducibles.
\item $KB \cup \Delta \models O$: $\Delta$ must provide missing information needed to prove observation $O$.
\item $KB \cup \Delta \not\models \perp$: $\Delta$ must be consistent with the knowledge base.
\item $KB \cup \Delta \models IC$: $\Delta$ when combined with the knowledge base must entail the constraints.
\end{itemize}

\paragraph{} When $KB$ is a set of definite clauses, the augmented theory $KB \cup \Delta$ will also be a set of definite clauses that accepts a unique minimal model, the Least Herbrand Model.

\subsection{Abductive Proof Procedure}

\subsubsection{Abductive Phase}\label{sec:AbductiveProofProcedureAbductivePhase}

\paragraph{} Let $<KB, Ab, IC>$ be an abductive model expressed in Normal Clausal Logic and let $O$ be a ground observation. To start the procedure, we let $G_1 = O$ and $\Delta_0$ initially be $\emptyset$:

\paragraph{} Select a subgoal $L \in G_i$, then let $G_i' = G_i - \{L\}$:
\begin{itemize}
\item $L \not\in Ab$ and $L$ is a non-negative atom:\\ If $\exists [(H \Leftarrow B) \in KB]$ st $L = H\theta$,\\ then $G_{i+1} = B\theta \cup G_i$ and $\Delta_{k+1} = \Delta_k$
\item $L \in \Delta_i$: $G_{i+1} = G_i$ and $\Delta_{k+1} = \Delta_k$
\item $L \in Ab$ and $L \not\in \Delta_k$ and $(\text{not}\ L) \not\in \Delta_k$: $L$ can be assumed but needs to go through \textit{Consistency Phase} (Section \ref{sec:AbductiveProofProcedureConsistencyPhase}) to verify that assumption does not introduce inconsistency. If the consistency phase succeeds\footnote{Succeds with a failure in derivation.}, then $\Delta_{k+1} = \Delta_k \cup {L}$.
\end{itemize}

\subsubsection{Consistency Phase}\label{sec:AbductiveProofProcedureConsistencyPhase}

\paragraph{} To prove that an assumption $A$ does not introduce inconsistency, the Consistency Phase requires the derivation to finish with a failure.

\paragraph{} Let $F_1$ be the set of all denials in $IC$ that has been resolved with assumption $A$. Select a denial $\Leftarrow \phi$ in $F_1$ and a literal L from $\phi$.

\begin{itemize}
\item $L \not\in Ab$: Perform SLDNF failure with L as a subgoal.
\item $L \in \Delta_k$: $\phi' = \phi - \{L\}$ and consider the new constraint $\Leftarrow \phi'$\
\item $L \in Ab$ and $(\text{not}\ L) \in \Delta_k$, then continue consistency phase with the next denial in $F_1$ to check.
\item $L \in Ab$ and $L \not\in \Delta_k$ and $(\text{not}\ L) \not\in \Delta_k$: As the literal has to fail, we perform an abductive derivation of its negation to check for its success (i.e. we go back to Abductive Phase, Section \ref{sec:AbductiveProofProcedureAbductivePhase}, with the subgoal $\text{not} L$). 
\end{itemize}

\section{Inductive Logic Programming}

\paragraph{} ILP can be seen as a search problem. In \textit{concept learning} the task is to compute the definition of a concept, expressed in a given language (i.e. hypotheses space), that satisfies all examples labelled as \textbf{positive} and none of the examples labelled as \textbf{negative} in a given dataset.

\paragraph{} However, the question is: how do we make the search for hypotheses that covers relation and quality criterion computationally feasible? A naive approach would be to perform a generate-and-test, which would become very inefficient when the version space gets large.

\subsection{Learning as a Search}

\paragraph{} Given 

\begin{itemize}
\setlength\itemsep{0.1em}
\item $\mathcal{L}_e$: a set of observations where
	\begin{itemize}
	\item $E^+$ is the set of positive examples
	\item $E^-$ is the set of negative examples
	\end{itemize}
\item $B$: Background knowledge / clausal theory
\item $\mathcal{L}_h$: hypothesis language
\item $c(B, \mathcal{L}_h, e)$: coverage of $\mathcal{L}_h$ over the example $e$
\end{itemize}

\noindent We want to find a theory $H \in \mathcal{L}_h$ st

\begin{itemize}
\setlength\itemsep{0.1em}
\item $\forall e \in E^+: B \cup H \models e$: H is \textbf{complete}
\item $\forall e \in E^-: B \cup H \not\models e$: H is \textbf{consistent}
\end{itemize}

\paragraph{} Using the same concept as concept learning, we need to define a notion of \textit{generality relation} between first-order theories and be able to put clauses in general-to-specific order.

\subsection{Generality of Theories}

\paragraph{} Let $C$ and $D$ be two definite clauses. 

\begin{defn} $C$ is more general than $D$ (denoted $C \geq_g D$) iff $C \models D$. If $C \not\models e$, then $D \not\models e$. \end{defn}

\begin{defn} $C$ subsumes $D$ iff $\exists \theta$ st $C\theta \subseteq D$. If $C$ subsumes $D$, $C \models D$. \end{defn}

\paragraph{$\theta$-subsumption} Given two sets of clauses where $H_1 = \{C_1,... , C_n\}$ and $H_2 = \{D_0,... , D_m\}$, to show that $H_1$ $\theta$-subumes $H_2$, we need to show that $\forall D_i$ clause $\in H_2$, $\exists C_i$ clause $\in H_1$ that subsumes $D_i$.

\paragraph{} A clause $C$ can $\theta$-subsume itself as the $\theta$-substitution would be $\theta = \{\}$. If an hypothesis is consistent (i.e. does not cover any negative example) then any specialisation of this hypothesis will also be consistent. Similarly if an hypothesis is complete (i.e. covers every positive example), then every generalisation of this hypothesis will also be complete. Recall we aim to find hypotheses that are complete and consistent.

\subsection{Lattice of Clauses}

\paragraph{} ILP for definite clauses can be described by i) Structure of the hypothesis space based on generality relation and ii) Search strategy (or prune strategy) by $\theta$-subsumption. The subsumption relation over definite clauses defines a lattice structure. There are two types of traversal over the lattice of hypotheses: specialization (going down the lattice to find a more specific hypothesis) and generalization (going up the lattice to find a more general hypothesis). Hence, ILP learning programs for definite clauses have been classified into either \textit{top-down} or \textit{bottom-up} learners.

\subsection{General-to-Specific Traversal}

\paragraph{Shapiro Operator} is a refinement / specialisation operator. The basic idea is to start from a set of positive and negative examples of a new concept and compute a set of Horn clauses that correctly represent this concept by traversing a lattice (i.e. a special version space) of possible hypothesis by means of the Shapiro ($\rho$) operator. The key idea is to add a litteral in the body of the current clause or apply a substitution $\theta$.

\paragraph{} TODO: Here I miss out Specific to General Traversal: Plotkin's least general generalisation of two clauses and Muggleton's inverse resolution.

\subsection{Learning from Interpretation}

\paragraph{} In \textit{Learning from Interpretation}, the set of examples contain a full description nad all the information that belongs to the example is represented in the example, not in the background knowledge. The Background only contains general information concerning the domain, not concerning specific examples. Hence a definite clause theory $H$ is a solution iff $e \in E^+$ is a model of $H$ (i.e. $e \models H$).

\subsection{Learning from Entailment}

\paragraph{} Learning from entailment is a different paradigm. Given the examples $E = <E^+, E^->$, each $e \in E$ is a ground fact and $B$ is the background definite clause theory. We want to find hypothesis $H$ st means $\forall e \in E^+\ B \cup H_i \models e$ and $\forall e \in E^-\ B \cup H_i \not\models e$

\section{HAIL}

\subsection{Predicate Learning}

\paragraph{} Predicate learning is classified into two kinds:

\begin{itemize}
\item \textbf{Observation Predicate Learning (OPL)}: Hypothesis and examples define the same predicates. 
\item \textbf{Non-Observation Predicate Learning (NOPL)}: Hypothesis and examples define different predicates.
\end{itemize}

\paragraph{} Generally knowledge about problem domain is not complete and general principles may need to be learned in order to explain observed complex phenomena. Hence a NOPL would look for new general knowledge, used together with existing knowledge, to explain the observations. 

\subsection{Inverse Entailment}

\paragraph{} Consider the notion of Inverse Entailment\footnote{Introduced by Prof. Muggleton and first implemented in PROGOL}. It combines the advantages of both bottom-up\footnote{narrowing the search by starting from what is known already} and top-down\footnote{refining a general hypothesis to form a new one} searches. We observe that

\[ B \cup H \models E \equiv B \cup \lnot E \models \lnot H \]

\paragraph{} Note that since $H$ and $E$ are set of clauses, their variables are universally quantified. When negating clauses in these sets, the variables becomes existentially quantified - skolemised. 

\subsection{Bottom Set}

\paragraph{} Considering a positive example $e^+ \in E^+$, we can compute the (possibly infinite) set of ground literals that are derivable from (or entailed by) $B \land \lnot e^+$. These set of ground literals are considered as the negation of the Bottom Set, denoted $\lnot Bot(B, e^+)$. Let $h$ and $h$ to be single Horn clauses where

\begin{itemize}
\item $h \equiv l_1 \lor \lnot l_2 \lor ... \lor \lnot l_n$: Note that $l_1$ is the head of the clause.
\item $e^+ \equiv a_1 \lor \lnot a_2 \lor ... \lor \lnot a_m$: Note that $a_1$ is the head of the clause.
\end{itemize}

\paragraph{} We then have their negation with skolemisation:

\begin{itemize}
\item $\lnot h \equiv \lnot l_1 \land l_2\theta \land ... \land l_n\theta$: $\theta$ is a grounding by existential quantifier for $h$.
\item $\lnot e^+ \equiv \lnot a_1\delta \land a_2\delta \land ... \land a_m\delta$: $\delta$ is a grounding by existential quantifier for $h$.
\end{itemize}

\paragraph{} The Bottom set is then defined as:

$$Bot(B, e^+) = \{\lnot l_1\theta, \lnot l_2\theta, ..., \lnot l_n\theta\}$$

\paragraph{} The Bottom Set is the \textbf{most specific ground set of clauses} that is at the bottom of an hypothesis search space that explains the example $e^+$. Let $g(\lnot h_{\perp}) \subseteq \lnot Bot(B, e^+)$ and consequently $\lnot Bot(B, e^+) \models g(\lnot h_{\perp})$. Remember that all variables in the sets $g(\lnot h_{\perp})$ and $\lnot Bot(B, e^+)$ are existentially quantified. By contrapositive, we have $g(h_{\perp}) \models Bot(B, e^+)$. If we replace every constant in $g(h_{\perp})$ with a unique variable, we obtain a universally quantified clause $h_{\perp}$. This clause $h_{\perp}$ $\theta$-subsumes the Bottom Set (and hence $h_{\perp} \models Bot(B, e^+)$).

\paragraph{} $h_{\perp}$ corresponds to the \textbf{most specific unground hypothesis} at the bottom of the hypothesis search space (or lattice) that $\theta$-subsumes $Bot(B, e^+)$. 

\paragraph{} Computing the Bottom Set first allows for a more efficient search for hypotheses, as the search space can be limited to the sub-lattice space delimited by the $Bot(B, e^+)$ as the bottom-most element and the empty clause as the top element. Solutions will be clauses that $\theta$-subsume the Bottom Set $Bot(B, e^+)$.

\paragraph{} We can look at the Least Herbrand Model of the set of $B \cup {\lnot e^+}$ for the list of literals that can be entailed from $B \cup \{\lnot e^+\}$. Alternatively to prove that the set satisfy a clause $h$, we can also use proof by resolution to show that $B \land \lnot e^+ \land \lnot h \vdash_{res} [\ ]$.

\paragraph{Learning by Bottom Generalisation} Using the computed Bottom Set to generate the bottom clause $h_{\perp}$, the next step would be to compute a more generate clause that subsume these bottom set through top-down refinement. We say that an hypothesis $H$ is derivable from the bottom generalisation from $B$ and $e^+$ iff $H \geq_g Bot(B, e^+)$. 

\subsection{Inverse Entailment \& NOPL}

\paragraph{} By definition of Inverse Entailment and Bottom Generalisation, we only look for bottom definite clauses (i.e. the Bottom Set has to include at most one positive literal by definition of definite clauses). Learning by Bottom Generalisation is good for OPL if we assume a complete background knowledge. However, it would fail to compute hypothesis about predicates that are not directly observed. Consider background knowledge and the positive example $hasbeak(tweety)$:

\[
B = \begin{cases}
haspeak(X) \leftarrow bird(X)\\
bird(X) \leftarrow vulture(X)
\end{cases}
\]

\paragraph{} We work out the following steps according to Bottom Generalisation:

\begin{align*}
\lnot e^+ &= \lnot hasbeak(tweety)\\
B \land \lnot e^+ &= \{\\
& \lnot hasbeak(tweety)\\
& \land \lnot bird(tweety) \\
& \land \lnot vulture(tweety)\\
\}&\\
Bot(B, e^+) &= hasbeak(tweety)\\
& \lor bird(tweety)\\
& \lor vulture(tweety)\\
g(h_{\perp}) &= \{ hasbeak(tweety) \}\\
h_{\perp} &= \{ hasbeak(X) \}
\end{align*}

\subsection{Progol5}

\paragraph{} We observe that the hypothesis in the Learning program above we derive have predicates that are in the examples and does not learn new concepts. Since we want to explain the example in terms of other concepts, the background knowledge needs to provide the links. The key problem is how to compute information that is missing in the given background knowledge and use these new notions to generate new hypotheses. Prof. Muggleton proposed an extension of the Progol system that incorporates the notion of contrapositives used in FOL theorem proving\cite{muggleton00}.

\paragraph{} A clause with $n$ number of literals in its body is also equivalent to $n$ number of different clauses generated by contrapositive in the following way:

\begin{align*}
s &\leftarrow\\
p &\leftarrow q, r\\
&\leftarrow \lnot s\\
\lnot q &\leftarrow \lnot p, r\\
\lnot r &\leftarrow \lnot p, q\\
\end{align*}

\paragraph{} As it is invalid syntax for Prolog to have negated literals in the head of the rules, we can be creative by inventing new names that represent such negation, for example:

\begin{align*}
s &\leftarrow\\
p &\leftarrow q, r\\
&\leftarrow non\_s\\
non\_q &\leftarrow non\_p, r\\
non\_r &\leftarrow non\_p, q\\
\end{align*}

\paragraph{} Now the background knowledge includes their equivalent contrapositive clauses.

\subsection{Language Bias}

\paragraph{} Sometimes we would like to define the language of the hypothesis by using a language bias. It is composed of a set of \textbf{mode declarations}.

\[
\text{Mode declarations: }
\begin{cases}
	modeh(r, s)\\
	modeb(r, s)
\end{cases}
\]

\paragraph{modeh} indicates the predicate may appear as head predicate of the rules to learn.
\paragraph{modeb} indicates the predicate may appear as body predicate of the rules to learn.
\paragraph{r} represents a recall, an integer that indicates how many times the predicate may appear in a rule. '*' indicates that the predicate may appear any number of times in a rule.
\paragraph{s} is the scheme, which is a ground atom with the predicate's name, placemarkers $+t$ (input variable), $-t$ (output variable) and $\#t$ (constants) for unary type $t$. 

\paragraph{Example} For example consider the following mode declarations:

\begin{align*}
	&\text{modeh}(1, \text{grandfather}(+\text{person}, +\text{person}))\\
	&\text{modeb}(1, \text{father}(+\text{person}, -\text{person}))\\
	&\text{modeb}(1, \text{parent}(+\text{person}, +\text{person}))
\end{align*}

We can build a rule $grandfather(X, Y) \leftarrow father(X, Z), parent(Z, Y)$.

\subsection{Kernel Set}

\paragraph{} The notion of the BottomSet $Bot(B, e)$ can be generalised into a new notion called \textbf{Kernel Set}. The creation of Kernel Set uses a full abductive reasoning procedure that replaces the StartSet procedure used to derive $Bot(B, e)$. 

{\footnotesize
\begin{align*}
& Kernel(B, E) \\
&= \{a | a \in \Delta \land B \cup \Delta \models e\} \cup \{\lnot b_i | B \cup \{\lnot e\} \models b_i\} \\
&= \bigwedge \{b_i | B \cup \{\lnot e\} \models b_i\} \rightarrow \bigvee \{a | a \in \Delta \land B \cup \Delta \models e\} \models b_i\}
\end{align*}}

\paragraph{} Note from above that the body literals are still generated the same way as it did by Progol5, i.e. $B \cup \{\lnot e\} \models b$. From the Kernel Set (that is ground), we then construct the Kernel Set hypothesis (that is ungrounded) given by

\[ 
K = \begin{cases}
a_1 \leftarrow b_{11}, b_{21}, ..., b_{n1}\\
a_2 \leftarrow b_{12}, b_{22}, ..., b_{m2}\\
...\\
a_k \leftarrow b_{1k}, b_{2k}, ..., b_{hk}
\end{cases}
\]

\subsection{Hybrid Abductive Inductive Learning}

TODO: complete HAIL algorithm

\paragraph{} We have background knowledge $B$, positive examples $E^+$, negative examples $E^-$ and mocdel $M$. 
\begin{enumerate}
\item Abductive Step (replace STARTSET): What are the abducibles?
\item Deductive Step
\item Induction Step
\end{enumerate}

Kernel is the set of clauses . Generalisation shrinks the Kernel set. Head of clauses are from the abduction proof. In Induction Step can I find a $H$ that $\theta$-subsumes $K$ in the $\theta$-lattice. Generalization increases positive-example coverage but we must check that it does not cover any negative examples.

\section{Top-directed Abductive Learning}

TODO: write TAL section

\section{Stable Model Semantics}

\subsection{Definitions}

\paragraph{} A Normal Logic Program is set of rules / clauses, where each rule $R$ is in the form

\begin{lstlisting}
h :- b1, ..., bn, not c1, ..., not cm.
\end{lstlisting}

\noindent ... where $h$, $b_i$ and $c_i$ are all atoms. We define 

\begin{itemize}
\item $head(R) = h$
\item $body^+(R) = \{ b_1, ..., b_n \}$
\item $body^-(R) = \{ c_1, ..., c_m \}$
\end{itemize}

\paragraph{} For convenience, I define $body(R) = body^+(R) \cup body^-(R)$ to be the set of all atoms in the body of the clause.

\subsection{Prolog and Negation}

\paragraph{} Consider the following program:

\begin{lstlisting}
p :- not q.
q :- not p.
\end{lstlisting}

\paragraph{} What should Prolog return if queried "$p$"? While Prolog will loop infinitely, there are in fact multiple answers to the query. The value of p depends on q and the value of q depends on p. The stable model semantics\cite{gelfond88} is a different approach to solving normal logic programs. Instead of providing solutions to specific queries, the stable model semantics defines the set of "stable" models of the program. we will see that the example program above has the stable models $\{p\}$ and $\{q\}$.

\subsection{Grounding}

\paragraph{} The solving any normal logic program is to first ground the logic program. For any program $P$ with function symbols, there are infinitely many ground instances of each rule in $P$ (consider function symbol compositions and there can be infinitely many different compositions). We write $ground(P)$ to refer to the grounding of logic program $P$.

\paragraph{} Most of these grounding instances of each rule may be redundant instances whose bodies could never be satisfied. ASP solvers do not generate these redundant rules.

\subsection{Safety}

\paragraph{} In Prolog, floundering (see section \ref{sec:NormalClausalLogic}) is a problem because negative literals may contain a variable which is only ground by a positive literal occurring later in the body of the rule. Recall that in Prolog, rule evaluation is ordered top to bottom and left to right. For example, the following logic program flounders in Prolog as $X$ and $Y$ only gets instantiated in $r(X, Y)$, which happens after $not\ q(X, Y)$:

\begin{lstlisting}
p(X) :- not q(X, Y), r(X, Y).
r(a, b).
\end{lstlisting}

\paragraph{} ASP does not consider the ordering of the literals in a rule and hence safety is less of a problem in ASP.  However, ASP needs to ground the entire program and another form of safety is needed. ASP Solvers are restricted to working with "safe" rules only.

\begin{defn}\label{defn:SafeRule}
A rule $R$ is safe, if every variable in $R$ occurs in at least one atom in $body^+(R)$. 
\end{defn}

\paragraph{} Considering Definition \ref{defn:SafeRule}, take a look at the following examples:

\begin{itemize}
\item \lstinline{p(Z) :- not q(X, Y), r(X, Y).} - This rule is unsafe because the variable $Z$ does not occur in any of the atoms in $body^+(R)$.
\item \lstinline{p(X) :- not q(X, Y), r(X, Y).} - This rule is safe.
\item \lstinline{p(Y) :- not q(X, Y), r(Y, Y).} - This rule is unsafe because the variable $X$ does not occur in any of the atoms in $body^+(R)$.
\end{itemize}

\paragraph{} Nonetheless, in ASP even when we restrict to safe rules, for some logic program $P$, $ground(P)$ can still be infinite. For example:

\begin{lstlisting}
p(f(X)) :- p(X).
p(1).
\end{lstlisting}

\subsection{Herbrand Theory}

\subsubsection{Definite Logic Programs}

\paragraph{} For a \textbf{definite logic program} $P$, a Herbrand Interpretation $I$ of $P$ is an Herbrand Model if for all rule $R$ st

\begin{itemize}
\item each atom in $body^+(R)$ is true in $I$
\item each atom in $body^-(R)$ is false in $I$
\item $head(R)$ is true in $I$
\end{itemize}

\paragraph{} We can construct the Least Herbrand Model for a definite logic program $P$, denoted $M(P)$ by starting with the empty set $M = {}$. We then repeatedly add any atom $h$ to $M$M st $h$ is the head of a rule $R$ whose body is a subset of $M$, i.e. we add $head(R)$ to $M$ if $body(R) \subseteq M$. \textit{For definite programs with no loops, this is the same as what is provable using Prolog.}

\subsubsection{Normal Logic Programs}

\paragraph{} However when it comes to \textbf{normal logic programs}, generally there is no Least Herbrand Model. Consider the following example:

\begin{lstlisting}
p :- not q.
q :- not p.
\end{lstlisting}

\paragraph{} The set of Herbrand Interpretations is $\{\emptyset, \{p\}, \{q\}, \{p ,q\}\}$.

\begin{itemize}
\item In the case of $HI = \emptyset$, it does not satisfy the program since both $not\ p$ and $not\ q$ are satisfied but their heads $q$ and $p$ respectively are not satisfied. 
\item In the case of $HI = \{p\}$, it satisfies the the program and is a Herbrand model of the program.
\item In the case of $HI = \{q\}$, it satisfies the the program and is a Herbrand model of the program.
\item In the case of $HI = \{p, q\}$, it satisfies the the program and is a Herbrand model of the program. However because the earlier two HIs are subsets of this HI, this is not a minimal Herbrand Model.
\end{itemize}

\paragraph{} Hence, instead of having just one Least Herbrand Model, the program has both $\{p\}$ and $\{q\}$ as two Minimal Herbrand Models of the program.

\subsubsection{Supportability}

\paragraph{} Consider this program:

\begin{lstlisting}
p :- not p.
\end{lstlisting}

\paragraph{} The program has the set of Herbrand Interpretations: $\{\emptyset, \{p\}\}$. The $\emptyset$ HI does not satisfy the program while $\{p\}$ does, which makes it the Least Herbrand Model. However, the LHM $\{p\}$ is unsupported because $p$ has no support: it is not a head of any clauses in the program whose body is true in the Herbrand Model. In fact, there are no stable models of this program.

\begin{defn}A Herbrand Model $M(P)$ for a normal logic program $P$ is supported iff every atom $a \in M(P)$ is the head of a clause in $P$ those body $body(R)$ is also true in $M(P)$, i.e. $body(R) \subseteq M(P)$. \end{defn}

\subsection{Reduct}


\bibliographystyle{plain}
\bibliography{references}

\end{multicols}
\end{document}